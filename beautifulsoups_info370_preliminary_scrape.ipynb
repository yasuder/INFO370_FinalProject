{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Data from Kickstarter Website\n",
    "\n",
    "This script (currently) goes to the \"Projects We Love\" page of the Kickstarter website, scrapes all projects (currently has filters that limit the total count to 21), and writes the necessary variables to a csv file (beautifulsoups_info370_preliminary_rawData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "# import pandas as pd \n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### main()\n",
    "This is the main method that calls the necessary functions to scrape the Kickstarter projects (from the \"Projects We Love\" filtered page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    base_urls = ['https://www.kickstarter.com/discover/advanced?state=live&woe_id=0&staff_picks=1&raised=1&sort=end_date&seed=2568554&page=']\n",
    "    for url in base_urls:\n",
    "        project_urls = get_pages(url)\n",
    "        scrape_pages(project_urls)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get_pages()\n",
    "@param base_url : the base url for the overview page that lists all projects of that category (to append page numbers to iterate through pages) <br>\n",
    "@return page_urls : the list of all project urls from each page of the base url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pages(base_url):\n",
    "    page = 1\n",
    "    valid = True\n",
    "    page_urls = []\n",
    "    while valid:\n",
    "        r = requests.get(base_url + str(page))\n",
    "        category_soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        results = category_soup.find_all('div', attrs={'class':'js-react-proj-card col-full col-sm-12-24 col-lg-8-24'})\n",
    "        if len(results) > 0:\n",
    "            for result in results:\n",
    "                data_project_json = json.loads(str(result['data-project']))\n",
    "                project_url = data_project_json['urls']['web']['project']\n",
    "                page_urls.append(project_url)\n",
    "#                 project_url = result.find('a', attrs={'class':'block img-placeholder w100p'})\n",
    "#                 print(project_url.get('href'))\n",
    "            page = page + 1\n",
    "        else:\n",
    "            valid = False\n",
    "    return page_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scrape_pages()\n",
    "@param urls : list of all project urls to scrape from <br>\n",
    "Writes all of the scraped data into a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_pages(urls):\n",
    "    pages = []\n",
    "    for url in urls:\n",
    "        r = requests.get(url)  \n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        top_portion = soup.find('div', attrs={'class':'bg-grey-100'})\n",
    "        attributes = get_attributes(top_portion, url)\n",
    "        attributes.append(get_update_count(soup))\n",
    "        reward_level_info = get_reward_levels(soup)\n",
    "        attributes.append(len(reward_level_info[0]))\n",
    "        attributes.append(reward_level_info[0]) # reward levels\n",
    "        attributes.append(reward_level_info[1]) #backers for each reward level\n",
    "        pages.append(attributes)\n",
    "    write_to_csv('beautifulsoups_info370_preliminary_rawData', pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get_attributes()\n",
    "@param intro_soup : the BeautifulSoup object of the introductory portion of the project page <br>\n",
    "@param url : url of the current page being scraped <br>\n",
    "@return records : the list of all scraped variables for the current project page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attributes(intro_soup, url):\n",
    "    records = []\n",
    "    data_initial_json = json.loads(str(intro_soup['data-initial']))\n",
    "    project = data_initial_json['project']\n",
    "    records.append(project['pid'])\n",
    "    records.append(project['name'])\n",
    "    records.append(url)\n",
    "    records.append(\"X\")\n",
    "#     records.append(project['category']['parentCategory']['name'])\n",
    "    records.append(project['category']['name'])\n",
    "    records.append(project['location']['displayableName'])\n",
    "    records.append(project['state'])\n",
    "    records.append(project['goal']['amount'])\n",
    "    records.append(project['pledged']['amount'])\n",
    "    records.append(project['percentFunded'])\n",
    "    records.append(project['backersCount'])\n",
    "    records.append(\"X\")\n",
    "    records.append(project['commentsCount'])\n",
    "    records.append(project['duration'])\n",
    "    return records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### write_to_csv()\n",
    "@param file_name : the name of the csv file to store the scraped project data in <br>\n",
    "@param values : the scraped values to be stored in the csv file <br>\n",
    "Stores the given data into a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(file_name, values):\n",
    "    col_names = ['project id', 'name', 'url', 'category', 'subcategory', 'location', 'status', 'goal', 'pledged', 'funded percent', 'backers', \n",
    "                 'funded date', 'comments', 'duration', 'updates', 'levels', 'reward levels', 'backers per reward level']\n",
    "    file=open(file_name + '.csv','w')\n",
    "    writer=csv.writer(file)\n",
    "    writer.writerow(col_names)\n",
    "    for row in values:\n",
    "        writer.writerow(row)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get_reward_levels()\n",
    "@param soup : the BeautifulSoup object of the project page\n",
    "@return reward_levels : the list of different pledge amounts for each reward level\n",
    "@return backers_by_levels : the list of the number of backers per reward level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward_levels(soup):\n",
    "    reward_levels = []\n",
    "    backers_by_levels = []\n",
    "    level_divs = soup.find_all('li', attrs={'class':'hover-group js-reward-available pledge--available pledge-selectable-sidebar'})\n",
    "    for level in level_divs:\n",
    "        price = level.find('span', attrs={'class':'money'}).text\n",
    "        backers = level.find('span', attrs={'class':'pledge__backer-count'}).text.replace('backers', '').strip()\n",
    "        reward_levels.append(price)\n",
    "        backers_by_levels.append((price, backers))\n",
    "    gone_level_divs = soup.find_all('li', attrs={'class':'hover-group pledge--all-gone pledge-selectable-sidebar'})\n",
    "    for level in gone_level_divs:\n",
    "        price = level.find('span', attrs={'class':'money'}).text\n",
    "        backers = level.find('span', attrs={'class':'pledge__backer-count'}).text.replace('backers', '').strip()\n",
    "        reward_levels.append(price)\n",
    "        backers_by_levels.append((price, backers))\n",
    "    return (reward_levels, backers_by_levels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get_update_count()\n",
    "@param soup : the BeautifulSoup object of the project page <br>\n",
    "@return updates : the number of updates that the project has undergone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_update_count(soup):\n",
    "    updates = soup.find('a', \n",
    "                        attrs={'class': 'js-load-project-content js-load-project-updates mx3 project-nav__link--updates tabbed-nav__link type-14'})\n",
    "    return updates.find('span').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
